{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # OpenCV\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras import applications\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "# Other libs\n",
    "import glob\n",
    "import os, sys\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the data generator\n",
    "def bring_data_from_directory():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "            \"./\",\n",
    "            target_size=(224, 224),\n",
    "            batch_size=1,\n",
    "            class_mode='categorical',\n",
    "            classes=[\"Original\", \"Deepfakes\"],\n",
    "            shuffle=False)\n",
    "    return train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = bring_data_from_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 11s 0us/step\n",
      "VGG16 CNN model loaded.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build Convolutional Neural Network model (VGG16, pretrained on ImageNet)\n",
    "def load_CNN_model():\n",
    "    CNN_model = VGG16(weights='imagenet', \n",
    "                      include_top=False, \n",
    "                      input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Retrained weights\n",
    "    #weights_path = \"/content/drive/My Drive/Hackathon for Good/Retrained CNN/retrainedCNN.h5\"\n",
    "    #CNN_model.load_weights(weights_path)\n",
    "    print(\"VGG16 CNN model loaded.\")\n",
    "    print(CNN_model.summary())\n",
    "    return CNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature extraction model\n",
    "CNN_model = load_CNN_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VIDEOS = 1000\n",
    "x_generator = None\n",
    "y_lable = None\n",
    "batch = 0\n",
    "TIMESTEPS = 10\n",
    "\n",
    "for x,y in train_generator:\n",
    "    if batch == int(N_VIDEOS*TIMESTEPS):\n",
    "        break\n",
    "\n",
    "    print(\"predict on batch:\", batch)\n",
    "    batch+=1\n",
    "    if x_generator is None:\n",
    "        x_generator = CNN_model.predict_on_batch(x)\n",
    "        y_label = y\n",
    "    else:\n",
    "        x_generator = np.append(x_generator, CNN_model.predict_on_batch(x), axis=0)\n",
    "        y_label = np.append(y_label, y, axis=0)\n",
    "\n",
    "#np.save(open('/content/drive/My Drive/Hackathon for Good/VGG_features/video_x_train_VGG16_original.npy', 'wb'), x_generator)\n",
    "#np.save(open('/content/drive/My Drive/Hackathon for Good/VGG_features/video_y_train_VGG16_original.npy','wb'), y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input videos\n",
    "TIMESTEPS = 10\n",
    "train_fake = np.load(open('video_x_train_VGG16_deepfake.npy','rb'))\n",
    "train_fake_labels = np.load(open('video_y_train_VGG16_deepfake.npy','rb'))\n",
    "train_original = np.load(open('video_x_train_VGG16_original.npy','rb'))\n",
    "train_original_labels = train_fake_labels.copy()\n",
    "train_original_labels[:,[0, 1]] = train_original_labels[:,[1, 0]]\n",
    "train_original_labels = train_original_labels[0:train_original.shape[0], :]\n",
    "\n",
    "train_data = np.concatenate((train_fake, train_original), axis=0)\n",
    "train_labels = np.concatenate((train_fake_labels, train_original_labels), axis=0)\n",
    "\n",
    "N_CLASSES = 2\n",
    "train_data = train_data.reshape(int(train_data.shape[0]/TIMESTEPS), \n",
    "                                TIMESTEPS,\n",
    "                                train_data.shape[1] * train_data.shape[2] * train_data.shape[3])\n",
    "train_labels = train_labels.reshape(int(train_labels.shape[0]/TIMESTEPS) , TIMESTEPS, N_CLASSES)[:, 0]\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(train_data, train_labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Recurrent Neural Network model (Long Short-Term Memory network)\n",
    "import tensorflow\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras import applications\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Other libs\n",
    "import glob\n",
    "import os, sys\n",
    "from tensorflow.keras.layers import TimeDistributed, Flatten\n",
    "\n",
    "batch_size = 300\n",
    "def train_lstm_model(train_data, train_labels, H_DIM=128):\n",
    "    ''' \n",
    "    Used to train the recurrent neural network model, using RMSprop optimizer\n",
    "\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(H_DIM,\n",
    "                   dropout=0.1,\n",
    "                   input_shape=(train_data.shape[1], train_data.shape[2])))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    optimizer = RMSprop()\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_loss', \n",
    "                             patience=10, \n",
    "                             verbose=0), \n",
    "               ModelCheckpoint('video_1_LSTM_1_1024.h5', \n",
    "                               monitor='val_loss', \n",
    "                               save_best_only=True, \n",
    "                               verbose=0) ]\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    model.fit(train_data, \n",
    "              train_labels,\n",
    "              validation_split=0.2,\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epoch,\n",
    "              callbacks=callbacks,\n",
    "              shuffle=True)\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120 samples, validate on 280 samples\n",
      "Epoch 1/100\n",
      "1120/1120 [==============================] - 20s 18ms/sample - loss: 1.3214 - acc: 0.4884 - val_loss: 0.7950 - val_acc: 0.4429\n",
      "Epoch 2/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7159 - acc: 0.4973 - val_loss: 0.7296 - val_acc: 0.4429\n",
      "Epoch 3/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7122 - acc: 0.5107 - val_loss: 0.7262 - val_acc: 0.4429\n",
      "Epoch 4/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7156 - acc: 0.4991 - val_loss: 0.7465 - val_acc: 0.4429\n",
      "Epoch 5/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.6984 - acc: 0.5232 - val_loss: 0.7132 - val_acc: 0.4429\n",
      "Epoch 6/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7180 - acc: 0.5107 - val_loss: 0.9068 - val_acc: 0.4429\n",
      "Epoch 7/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7558 - acc: 0.5098 - val_loss: 0.7126 - val_acc: 0.4429\n",
      "Epoch 8/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7119 - acc: 0.4964 - val_loss: 0.7630 - val_acc: 0.4429\n",
      "Epoch 9/100\n",
      "1120/1120 [==============================] - 7s 7ms/sample - loss: 0.7233 - acc: 0.4973 - val_loss: 0.7268 - val_acc: 0.4429\n",
      "Epoch 10/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7328 - acc: 0.4830 - val_loss: 0.7972 - val_acc: 0.4429\n",
      "Epoch 11/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7152 - acc: 0.5116 - val_loss: 0.6883 - val_acc: 0.5571\n",
      "Epoch 12/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7001 - acc: 0.5179 - val_loss: 0.6870 - val_acc: 0.5571\n",
      "Epoch 13/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7470 - acc: 0.4920 - val_loss: 0.6904 - val_acc: 0.5571\n",
      "Epoch 14/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.6944 - acc: 0.5250 - val_loss: 0.7197 - val_acc: 0.4429\n",
      "Epoch 15/100\n",
      "1120/1120 [==============================] - 7s 7ms/sample - loss: 0.7100 - acc: 0.5000 - val_loss: 0.7454 - val_acc: 0.4429\n",
      "Epoch 16/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7405 - acc: 0.4911 - val_loss: 0.7365 - val_acc: 0.4429\n",
      "Epoch 17/100\n",
      "1120/1120 [==============================] - 7s 7ms/sample - loss: 0.7384 - acc: 0.4777 - val_loss: 0.7023 - val_acc: 0.4429\n",
      "Epoch 18/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7025 - acc: 0.4884 - val_loss: 0.7272 - val_acc: 0.4429\n",
      "Epoch 19/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7115 - acc: 0.5143 - val_loss: 0.7036 - val_acc: 0.5571\n",
      "Epoch 20/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7200 - acc: 0.4955 - val_loss: 0.6874 - val_acc: 0.5571\n",
      "Epoch 21/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.6918 - acc: 0.5312 - val_loss: 0.7149 - val_acc: 0.4429\n",
      "Epoch 22/100\n",
      "1120/1120 [==============================] - 8s 7ms/sample - loss: 0.7192 - acc: 0.5027 - val_loss: 0.6898 - val_acc: 0.5571\n"
     ]
    }
   ],
   "source": [
    "# Train the recurrent neural network model\n",
    "lstm_model = train_lstm_model(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN_model.save(\"CNN_model.h5\")\n",
    "#lstm_model.save(\"LSTM_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
